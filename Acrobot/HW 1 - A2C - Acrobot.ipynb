{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e3c459faaa16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# render = lambda : plt.imshow(env.render(mode='rgb_array'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error occured while running `from pyglet.gl import *`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcarbon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCarbonConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;31m# XXX remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "# render = lambda : plt.imshow(env.render(mode='rgb_array'))\n",
    "# render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(seed):\n",
    "    def _func():\n",
    "        env = gym.make('Acrobot-v1')\n",
    "        env.seed(seed)\n",
    "        obs_shape = env.observation_space.shape\n",
    " \n",
    "        return env\n",
    "    return _func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "gamma = 0.99\n",
    "num_steps = 2e6\n",
    "rollout_steps = 40\n",
    "hidden_layer = 50\n",
    "\n",
    "num_processes = 64\n",
    "entropy_start = 10.0\n",
    "entropy_end = 0\n",
    "entropy_decay_steps = 1e6\n",
    "\n",
    "value_coeff = 0.5\n",
    "l1_regularization = 0\n",
    "dropout = 0\n",
    "lambd = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = [make_env(i) for i in range(num_processes)]\n",
    "if len(envs) > 1:\n",
    "    envs = SubprocVecEnv(envs)\n",
    "else:\n",
    "    envs = DummyVecEnv(envs)\n",
    "\n",
    "input_size = envs.observation_space.n\n",
    "output_size = envs.action_space.n\n",
    "    \n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACTOR_MLP(nn.Module):\n",
    "    def __init__(self, in_size, out_size, hidden_size, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(in_size, hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.lin2 = nn.Linear(hidden_size, out_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout1(F.relu(self.lin1(x)))\n",
    "        x = self.dropout2(self.lin2(x))\n",
    "        return x\n",
    "    \n",
    "class CRITIC_MLP(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(in_size, hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.lin2 = nn.Linear(hidden_size, 1)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout1(F.relu(self.lin1(x)))\n",
    "        return self.dropout2(self.lin2(x))\n",
    "\n",
    "def init_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = np.prod(weight_shape[1:4])\n",
    "        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = weight_shape[1]\n",
    "        fan_out = weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = ACTOR_MLP(input_size, output_size, hidden_layer, dropout)\n",
    "actor.apply(init_weights)\n",
    "\n",
    "critic = CRITIC_MLP(input_size, hidden_layer, dropout)\n",
    "critic.apply(init_weights)\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=lr)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_groups(x, y, group_size):\n",
    "    num_groups = int(len(x) / group_size)\n",
    "\n",
    "    x, x_tail = x[:group_size * num_groups], x[group_size * num_groups:]\n",
    "    x = x.reshape((num_groups, group_size))\n",
    "\n",
    "    y, y_tail = y[:group_size * num_groups], y[group_size * num_groups:]\n",
    "    y = y.reshape((num_groups, group_size))\n",
    "\n",
    "    x_means = x.mean(axis=1)\n",
    "    x_stds = x.std(axis=1)\n",
    "\n",
    "    if len(x_tail) > 0:\n",
    "        x_means = np.concatenate([x_means, x_tail.mean(axis=0, keepdims=True)])\n",
    "        x_stds = np.concatenate([x_stds, x_tail.std(axis=0, keepdims=True)])\n",
    "\n",
    "    y_means = y.mean(axis=1)\n",
    "    y_stds = y.std(axis=1)\n",
    "\n",
    "    if len(y_tail) > 0:\n",
    "        y_means = np.concatenate([y_means, y_tail.mean(axis=0, keepdims=True)])\n",
    "        y_stds = np.concatenate([y_stds, y_tail.std(axis=0, keepdims=True)])\n",
    "\n",
    "    return x_means, x_stds, y_means, y_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_state(indices):\n",
    "    state = torch.zeros([num_processes, input_size], dtype=torch.float32)\n",
    "    state[[i for i in range(num_processes)], indices] = 1\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rollout(steps):\n",
    "    # bootstrap discounted returns with final value estimates\n",
    "    _, _, _, _, last_values = steps[-1]\n",
    "    returns = last_values.data\n",
    "\n",
    "    advantages = torch.zeros(num_processes, 1)\n",
    "\n",
    "    out = [None] * (len(steps) - 1)\n",
    "\n",
    "    # run Generalized Advantage Estimation, calculate returns, advantages\n",
    "    for t in reversed(range(len(steps) - 1)):\n",
    "        rewards, masks, actions, policies, values = steps[t]\n",
    "        _, _, _, _, next_values = steps[t + 1]\n",
    "\n",
    "        returns = rewards + returns * gamma * masks\n",
    "\n",
    "        deltas = rewards + next_values.data * gamma * masks - values.data\n",
    "        advantages = advantages * gamma * lambd * masks + deltas\n",
    "\n",
    "        out[t] = actions, policies, values, returns, advantages\n",
    "\n",
    "    # return data as batched Tensors, Variables\n",
    "    return map(lambda x: torch.cat(x, 0), zip(*out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_idx = envs.reset()\n",
    "\n",
    "total_steps_plt = []\n",
    "ep_reward_plt = []\n",
    "\n",
    "steps = []\n",
    "total_steps = 0\n",
    "ep_rewards = [0.] * num_processes\n",
    "render_timer = 0\n",
    "plot_timer = 0\n",
    "while total_steps < num_steps:\n",
    "    for _ in range(rollout_steps):\n",
    "        obs = make_state(state_idx)\n",
    "\n",
    "        # network forward pass\n",
    "        policies = actor(obs)\n",
    "        probs = F.softmax(policies)\n",
    "        actions = probs.multinomial(1).data\n",
    "\n",
    "        values = critic(obs)\n",
    "\n",
    "        # gather env data, reset done envs and update their obs\n",
    "        state_idx, rewards, dones, _ = envs.step(actions.squeeze(-1).cpu().numpy())\n",
    "        masks = (1. - torch.from_numpy(np.array(dones, dtype=np.float32))).unsqueeze(1)\n",
    "\n",
    "        total_steps += num_processes\n",
    "        for i, done in enumerate(dones):\n",
    "            ep_rewards[i] += rewards[i]\n",
    "            if done:\n",
    "                total_steps_plt.append(total_steps)\n",
    "                ep_reward_plt.append(ep_rewards[i])\n",
    "                ep_rewards[i] = 0\n",
    "\n",
    "        plot_timer += num_processes # time on total steps\n",
    "        if plot_timer >= 100000:\n",
    "            clear_output()\n",
    "            x_means, _, y_means, y_stds = mean_std_groups(np.array(total_steps_plt), np.array(ep_reward_plt), 10)\n",
    "            fig = plt.figure()\n",
    "            fig.set_size_inches(8, 6)\n",
    "            plt.ticklabel_format(axis='x', style='sci', scilimits=(-2, 6))\n",
    "            plt.errorbar(x_means, y_means, yerr=y_stds, ecolor='xkcd:blue', fmt='xkcd:black', capsize=5, elinewidth=1.5, mew=1.5, linewidth=1.5)\n",
    "            plt.title('Training progress (%s)' % 'Taxi-v2')\n",
    "            plt.xlabel('Total steps')\n",
    "            plt.ylabel('Episode reward')\n",
    "            plt.show()\n",
    "            plot_timer = 0\n",
    "            \n",
    "            print('Mean: ' + str(y_means[-1]) + ', Std: ' + str(y_stds[-1]))\n",
    "\n",
    "        rewards = torch.from_numpy(rewards).float().unsqueeze(1)\n",
    "\n",
    "        steps.append((rewards, masks, actions, policies, values))\n",
    "\n",
    "    final_obs = make_state(state_idx)\n",
    "    final_values = critic(final_obs)\n",
    "    steps.append((None, None, None, None, final_values))\n",
    "\n",
    "    actions, policies, values, returns, advantages = process_rollout(steps)\n",
    "\n",
    "    # calculate action probabilities\n",
    "    probs = F.softmax(policies)\n",
    "    log_probs = F.log_softmax(policies)\n",
    "    log_action_probs = log_probs.gather(1, actions.detach())\n",
    "\n",
    "    policy_loss = (-log_action_probs * advantages.detach()).sum()\n",
    "    value_loss = (.5 * (values - returns.detach()) ** 2.).sum()\n",
    "    entropy_loss = (log_probs * probs).sum()\n",
    "\n",
    "    entropy_coeff = max(entropy_end, entropy_start * (1 - total_steps * 1.0 / entropy_decay_steps) + entropy_end * total_steps * 1.0 / entropy_decay_steps)\n",
    "    \n",
    "    actor_loss = policy_loss + entropy_loss * entropy_coeff\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "    actor_optimizer.zero_grad()\n",
    "    \n",
    "    critic_loss = value_loss * value_coeff\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "    critic_optimizer.zero_grad()\n",
    "\n",
    "    steps = []\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(actor.state_dict(), 'a2c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.load_state_dict(torch.load('a2c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make('Acrobot-v1')\n",
    "eval_state_idx = eval_env.reset()\n",
    "while True:\n",
    "    eval_env.render()\n",
    "    eval_state = torch.zeros([input_size], dtype=torch.float32)\n",
    "    eval_state[eval_state_idx] = 1\n",
    "\n",
    "    policies = actor(eval_state.unsqueeze(0))\n",
    "    probs = F.softmax(policies)\n",
    "    actions = probs.multinomial(1).data\n",
    "    eval_state_idx, reward, done, _ = eval_env.step(actions.item())\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "eval_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rews = []\n",
    "for _ in range(1000):\n",
    "    eval_state_idx = eval_env.reset()\n",
    "    rew = 0\n",
    "    while True:\n",
    "        eval_state = torch.zeros([input_size], dtype=torch.float32)\n",
    "        eval_state[eval_state_idx] = 1\n",
    "\n",
    "        policies = actor(eval_state.unsqueeze(0))\n",
    "        probs = F.softmax(policies)\n",
    "        actions = probs.multinomial(1).data\n",
    "    \n",
    "        eval_state_idx, reward, done, _ = eval_env.step(actions.item())\n",
    "\n",
    "        rew += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    rews.append(rew)\n",
    "print('Average reward: ' + str(np.mean(rews)))\n",
    "print('STD: ' + str(np.std(rews)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
