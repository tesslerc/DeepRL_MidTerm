{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from gym import Wrapper\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_screen(_envs):\n",
    "    state = []\n",
    "    for env in _envs.envs:\n",
    "        # Get the rgb array, remove the top 120 pixels (white space irrelevant for the task).\n",
    "        screen = env.render(mode='rgb_array')[120:, :, :]\n",
    "        # Convert to PIL image.\n",
    "        image = Image.fromarray(screen, 'RGB')\n",
    "        # Resize to 64x64 and convert to grayscale.\n",
    "        image = image.resize((64, 64), Image.ANTIALIAS).convert('L')\n",
    "        # Invert (turn black to white...) and normalize to [0, 1]\n",
    "        state.append((255 - np.asarray(image.getdata()).reshape(64, 64) * 1.0) / 255)\n",
    "    return torch.Tensor(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment creation helper function (for vec env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(seed):\n",
    "    def _func():\n",
    "        env = gym.make('Acrobot-v1')\n",
    "        env.seed(seed)\n",
    "        obs_shape = env.observation_space.shape\n",
    " \n",
    "        return env\n",
    "    return _func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General parameters.\n",
    "mem_capacity = 20000\n",
    "lr = 0.001\n",
    "gamma = 0.99\n",
    "\n",
    "# Number of parallel agents.\n",
    "num_processes = 8\n",
    "\n",
    "# How many rollout steps before taking the gradients (e.g., monte-carlo sampling trajectory length).\n",
    "rollout_steps = 512\n",
    "\n",
    "# Decay entropy coeff linearly over entropy_decay_steps steps, starting from entropy_start down to entropy_end.\n",
    "entropy_start = 0.01\n",
    "entropy_end = 1e-6\n",
    "entropy_decay_steps = 1e6\n",
    "\n",
    "# Critic coefficient value.\n",
    "value_coeff = 0.5\n",
    "\n",
    "# GAE coefficient.\n",
    "lambd = 0.95\n",
    "\n",
    "# Number of previous frames to stack in order to obtain a markovian representation.\n",
    "hist_len = 2\n",
    "\n",
    "# Previous experiments showed that Dropout did not help, thus we leave it turned off.\n",
    "dropout = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = [make_env(i) for i in range(num_processes)]\n",
    "envs = DummyVecEnv(envs)\n",
    "\n",
    "output_size = envs.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network definition (actor-critic with shared representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ACTOR_CRITIC(nn.Module):\n",
    "    def __init__(self, hist_len, out_size):\n",
    "        super().__init__()\n",
    "        self.hist_len = hist_len\n",
    "        self.out_size = out_size\n",
    "        \n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(hist_len, 64, kernel_size=8, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.v_projection = nn.Sequential(\n",
    "            nn.Linear(1568, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.policy_projection = nn.Sequential(\n",
    "            nn.Linear(1568, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.out_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extraction(x)\n",
    "        return self.v_projection(features), self.policy_projection(features)\n",
    "\n",
    "# Smarter weight initialization scheme, works better than the default one provided by PyTorch.\n",
    "def init_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = np.prod(weight_shape[1:4])\n",
    "        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = weight_shape[1]\n",
    "        fan_out = weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        mem_size = len(self.memory)\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*batch)\n",
    "        return batch_state, batch_action, batch_reward, batch_next_state, batch_done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize actor, critic, replay memory and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ACTOR_CRITIC(hist_len, output_size)\n",
    "agent.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(agent.parameters(), lr=lr)\n",
    "\n",
    "memory = ReplayBuffer(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given samples, calculates the running mean and STD for better visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_groups(x, y, group_size):\n",
    "    num_groups = int(len(x) / group_size)\n",
    "\n",
    "    x, x_tail = x[:group_size * num_groups], x[group_size * num_groups:]\n",
    "    x = x.reshape((num_groups, group_size))\n",
    "\n",
    "    y, y_tail = y[:group_size * num_groups], y[group_size * num_groups:]\n",
    "    y = y.reshape((num_groups, group_size))\n",
    "\n",
    "    x_means = x.mean(axis=1)\n",
    "    x_stds = x.std(axis=1)\n",
    "\n",
    "    if len(x_tail) > 0:\n",
    "        x_means = np.concatenate([x_means, x_tail.mean(axis=0, keepdims=True)])\n",
    "        x_stds = np.concatenate([x_stds, x_tail.std(axis=0, keepdims=True)])\n",
    "\n",
    "    y_means = y.mean(axis=1)\n",
    "    y_stds = y.std(axis=1)\n",
    "\n",
    "    if len(y_tail) > 0:\n",
    "        y_means = np.concatenate([y_means, y_tail.mean(axis=0, keepdims=True)])\n",
    "        y_stds = np.concatenate([y_stds, y_tail.std(axis=0, keepdims=True)])\n",
    "\n",
    "    return x_means, x_stds, y_means, y_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given \"rollout_steps\" samples, calculate advantages and deltas for actor-critic training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rollout(steps):\n",
    "    # bootstrap discounted returns with final value estimates\n",
    "    _, _, _, _, last_values = steps[-1]\n",
    "    returns = last_values.data\n",
    "\n",
    "    advantages = torch.zeros(num_processes, 1)\n",
    "\n",
    "    out = [None] * (len(steps) - 1)\n",
    "\n",
    "    # run Generalized Advantage Estimation, calculate returns, advantages\n",
    "    for t in reversed(range(len(steps) - 1)):\n",
    "        rewards, masks, actions, policies, values = steps[t]\n",
    "        _, _, _, _, next_values = steps[t + 1]\n",
    "\n",
    "        returns = rewards + returns * gamma * masks\n",
    "\n",
    "        deltas = rewards + next_values.data * gamma * masks - values.data\n",
    "        advantages = advantages * gamma * lambd * masks + deltas\n",
    "\n",
    "        out[t] = actions, policies, values, returns, advantages\n",
    "\n",
    "    # return data as batched Tensors, Variables\n",
    "    return map(lambda x: torch.cat(x, 0), zip(*out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "envs.reset()\n",
    "\n",
    "total_steps_plt = []\n",
    "ep_reward_plt = []\n",
    "\n",
    "steps = []\n",
    "total_steps = 0\n",
    "ep_rewards = [0.] * num_processes\n",
    "render_timer = 0\n",
    "plot_timer = 0\n",
    "\n",
    "state = torch.zeros((num_processes, hist_len, 64, 64)).float()\n",
    "obs = get_screen(envs)\n",
    "# pop-first observation and append new one (total hist_len observations per state)\n",
    "state[:, :-2, :, :] = state[:, 1:-1, :, :]\n",
    "state[:, -1, :, :] = obs\n",
    "\n",
    "while total_steps < num_steps:\n",
    "    for _ in range(rollout_steps):\n",
    "        values, policies = agent(state.detach().clone())\n",
    "        probs = F.softmax(policies, dim=-1)\n",
    "        # Sample action from stochastic policy\n",
    "        actions = probs.multinomial(1).data\n",
    "\n",
    "        _, rewards, dones, _ = envs.step(actions.squeeze(-1).cpu().numpy())\n",
    "        obs = get_screen(envs)\n",
    "        \n",
    "        next_state = state.clone()\n",
    "        next_state[:, :-2, :, :] = next_state[:, 1:-1, :, :]\n",
    "        next_state[:, -1, :, :] = obs\n",
    "        \n",
    "        for i in range(num_processes):\n",
    "            memory.add(state[i], actions[i], rewards[i], next_state[i], dones[i])\n",
    "                \n",
    "        state = next_state.clone()\n",
    "        \n",
    "        for i in range(num_processes):\n",
    "            if dones[i] and rewards[i] < 0:\n",
    "                rewards[i] = values[i].data\n",
    "        masks = (1. - torch.from_numpy(np.array(dones, dtype=np.float32))).unsqueeze(1)\n",
    "\n",
    "        total_steps += num_processes\n",
    "        # For each process done, add the episode reward to our list\n",
    "        for i, done in enumerate(dones):\n",
    "            ep_rewards[i] += rewards[i]\n",
    "            if done:\n",
    "                total_steps_plt.append(total_steps)\n",
    "                ep_reward_plt.append(ep_rewards[i])\n",
    "                ep_rewards[i] = 0\n",
    "\n",
    "        plot_timer += num_processes # time on total steps\n",
    "        if plot_timer >= 10000:  # Re-plot graphs\n",
    "            clear_output()\n",
    "            x_means, _, y_means, y_stds = mean_std_groups(np.array(total_steps_plt), np.array(ep_reward_plt), 10)\n",
    "            fig = plt.figure()\n",
    "            fig.set_size_inches(8, 6)\n",
    "            plt.ticklabel_format(axis='x', style='sci', scilimits=(-2, 6))\n",
    "            plt.errorbar(x_means, y_means, yerr=y_stds, ecolor='xkcd:blue', fmt='xkcd:black', capsize=5, elinewidth=1.5, mew=1.5, linewidth=1.5)\n",
    "            plt.title('Training progress (%s)' % 'Acrobot-v1')\n",
    "            plt.xlabel('Total steps')\n",
    "            plt.ylabel('Episode reward')\n",
    "            plt.show()\n",
    "            plot_timer = 0\n",
    "            \n",
    "            print('Mean: ' + str(y_means[-1]) + ', Std: ' + str(y_stds[-1]))\n",
    "            print(probs)\n",
    "\n",
    "        rewards = torch.from_numpy(rewards).float().unsqueeze(1)\n",
    "\n",
    "        steps.append((rewards, masks, actions, policies, values))\n",
    "\n",
    "    # At the final step of the rollout, add the bootstrap value\n",
    "    obs = get_screen(envs)\n",
    "    state[:, :-2, :, :] = state[:, 1:-1, :, :]\n",
    "    state[:, -1, :, :] = obs\n",
    "    final_values, _ = agent(state.detach().clone())\n",
    "    steps.append((None, None, None, None, final_values))\n",
    "\n",
    "    actions, policies, values, returns, advantages = process_rollout(steps)\n",
    "\n",
    "    # calculate action probabilities\n",
    "    probs = F.softmax(policies, dim=-1)\n",
    "    log_probs = F.log_softmax(policies, dim=-1)\n",
    "    log_action_probs = log_probs.gather(1, actions.detach())\n",
    "\n",
    "    policy_loss = (-log_action_probs * advantages.detach()).sum()\n",
    "    value_loss = F.smooth_l1_loss(values, returns.detach())\n",
    "    entropy_loss = (log_probs * probs).sum()\n",
    "\n",
    "    # get updated entropy coefficient\n",
    "    entropy_coeff = max(entropy_end, entropy_start * (1 - total_steps * 1.0 / entropy_decay_steps) + entropy_end * total_steps * 1.0 / entropy_decay_steps)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = policy_loss + entropy_loss * entropy_coeff + value_loss * value_coeff\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # self imitation learning\n",
    "    batch_state, batch_action, batch_reward, batch_next_state, not_done_mask = memory.sample(rollout_steps)\n",
    "    batch_state = torch.stack(batch_state).detach()\n",
    "    batch_next_state = torch.stack(batch_next_state).detach()\n",
    "    batch_action = torch.tensor(batch_action, dtype=torch.int64).unsqueeze(-1).detach()\n",
    "    batch_reward = torch.tensor(batch_reward, dtype=torch.float32).unsqueeze(-1).detach()\n",
    "    not_done_mask = torch.tensor(not_done_mask, dtype=torch.float32).unsqueeze(-1).detach()\n",
    "    \n",
    "    values, policies = agent(batch_state.detach().clone())\n",
    "    log_actions_probs = F.log_softmax(policies, dim=-1).gather(1, batch_action)\n",
    "    \n",
    "    next_state_values, _ = agent(batch_next_state.detach().clone())\n",
    "    \n",
    "    sil_mask = ((gamma * next_state_values.data * not_done_mask + batch_reward - values.data) > 0).float()\n",
    "    advantages = (gamma * next_state_values.data * not_done_mask + batch_reward - values.data) * sil_mask\n",
    "    \n",
    "    value_sil_loss = F.smooth_l1_loss(values, gamma * next_state_values.data * not_done_mask + batch_reward, reduce=False)\n",
    "    value_sil_loss = (value_sil_loss * sil_mask).mean()\n",
    "    \n",
    "    policy_loss = (-log_actions_probs * advantages.detach()).sum()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = policy_loss + value_sil_loss * value_coeff\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    steps = []\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(agent.state_dict(), 'a2c_acrobot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_state_dict(torch.load('a2c_acrobot'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate network (one run with visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = DummyVecEnv([make_env(0)])\n",
    "eval_env.reset()\n",
    "\n",
    "eval_state = torch.zeros((num_processes, hist_len, 64, 64)).float()\n",
    "while True:\n",
    "    obs = get_screen(eval_env)\n",
    "    eval_state[:, :-2, :, :] = eval_state[:, 1:-1, :, :]\n",
    "    eval_state[:, -1, :, :] = obs\n",
    "    \n",
    "    _, policies = agent(state.detach().clone())\n",
    "    probs = F.softmax(policies)\n",
    "    actions = probs.multinomial(1).data\n",
    "    _, reward, done, _ = eval_env.step(actions.squeeze(-1).cpu().numpy())\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "eval_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate network with mean and std outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rews = []\n",
    "for _ in range(1000):\n",
    "    eval_env.reset()\n",
    "    rew = 0\n",
    "    eval_state = torch.zeros((num_processes, hist_len, 64, 64)).float()\n",
    "    while True:\n",
    "        obs = get_screen(eval_env)\n",
    "        eval_state[:, :hist_len-1, :, :] = eval_state[:, 1:-1, :, :]\n",
    "        eval_state[:, -1, :, :] = obs\n",
    "\n",
    "        policies = actor(eval_state.unsqueeze(0))\n",
    "        probs = F.softmax(policies)\n",
    "        actions = probs.multinomial(1).data\n",
    "    \n",
    "        _, reward, done, _ = eval_env.step(actions.item())\n",
    "\n",
    "        rew += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    rews.append(rew)\n",
    "print('Average reward: ' + str(np.mean(rews)))\n",
    "print('STD: ' + str(np.std(rews)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
